{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "# https://github.com/petrosDemetrakopoulos/RNN-Beatles-lyrics-generator\n",
    "# https://github.com/starry91/Lyric-Generator#2-lyric-generator-based-on-word-level-rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>genre</th>\n",
       "      <th>title</th>\n",
       "      <th>lyrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Migos</td>\n",
       "      <td>rap</td>\n",
       "      <td>Stir Fry</td>\n",
       "      <td>Woo, woo, woo, woo\\nWoo, woo, woo, woo\\n\\nDanc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Snoop Dogg</td>\n",
       "      <td>rap</td>\n",
       "      <td>Drop It Like It‚Äôs Hot</td>\n",
       "      <td>Snoop\\nSnoop\\n\\nWhen the pimp's in the crib, m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Drake</td>\n",
       "      <td>rap</td>\n",
       "      <td>Headlines</td>\n",
       "      <td>I might be too strung out on compliments\\nOver...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Lil Uzi Vert</td>\n",
       "      <td>rap</td>\n",
       "      <td>XO TOUR Llif3</td>\n",
       "      <td>Are you alright?\\nI'm alright, I'm quite alrig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Lil Uzi Vert</td>\n",
       "      <td>rap</td>\n",
       "      <td>The Way Life Goes</td>\n",
       "      <td>That's true (That's true), that's right (That ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         artist genre                    title  \\\n",
       "0         Migos   rap                 Stir Fry   \n",
       "1    Snoop Dogg   rap  Drop It Like It‚Äôs Hot   \n",
       "2         Drake   rap                Headlines   \n",
       "3  Lil Uzi Vert   rap            XO TOUR Llif3   \n",
       "4  Lil Uzi Vert   rap        The Way Life Goes   \n",
       "\n",
       "                                              lyrics  \n",
       "0  Woo, woo, woo, woo\\nWoo, woo, woo, woo\\n\\nDanc...  \n",
       "1  Snoop\\nSnoop\\n\\nWhen the pimp's in the crib, m...  \n",
       "2  I might be too strung out on compliments\\nOver...  \n",
       "3  Are you alright?\\nI'm alright, I'm quite alrig...  \n",
       "4  That's true (That's true), that's right (That ...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"sample-dataset.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>genre</th>\n",
       "      <th>title</th>\n",
       "      <th>lyrics</th>\n",
       "      <th>t-lyric</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Migos</td>\n",
       "      <td>rap</td>\n",
       "      <td>Stir Fry</td>\n",
       "      <td>Woo, woo, woo, woo\\nWoo, woo, woo, woo\\n\\nDanc...</td>\n",
       "      <td>stir fry @@@ woo, woo, woo, woo\\nwoo, woo, woo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Snoop Dogg</td>\n",
       "      <td>rap</td>\n",
       "      <td>Drop It Like It‚Äôs Hot</td>\n",
       "      <td>Snoop\\nSnoop\\n\\nWhen the pimp's in the crib, m...</td>\n",
       "      <td>drop it like it‚äôs hot @@@ snoop\\nsnoop\\n\\nwh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Drake</td>\n",
       "      <td>rap</td>\n",
       "      <td>Headlines</td>\n",
       "      <td>I might be too strung out on compliments\\nOver...</td>\n",
       "      <td>headlines @@@ i might be too strung out on com...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Lil Uzi Vert</td>\n",
       "      <td>rap</td>\n",
       "      <td>XO TOUR Llif3</td>\n",
       "      <td>Are you alright?\\nI'm alright, I'm quite alrig...</td>\n",
       "      <td>xo tour llif3 @@@ are you alright?\\ni'm alrigh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Lil Uzi Vert</td>\n",
       "      <td>rap</td>\n",
       "      <td>The Way Life Goes</td>\n",
       "      <td>That's true (That's true), that's right (That ...</td>\n",
       "      <td>the way life goes @@@ that's true (that's true...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         artist genre                    title  \\\n",
       "0         Migos   rap                 Stir Fry   \n",
       "1    Snoop Dogg   rap  Drop It Like It‚Äôs Hot   \n",
       "2         Drake   rap                Headlines   \n",
       "3  Lil Uzi Vert   rap            XO TOUR Llif3   \n",
       "4  Lil Uzi Vert   rap        The Way Life Goes   \n",
       "\n",
       "                                              lyrics  \\\n",
       "0  Woo, woo, woo, woo\\nWoo, woo, woo, woo\\n\\nDanc...   \n",
       "1  Snoop\\nSnoop\\n\\nWhen the pimp's in the crib, m...   \n",
       "2  I might be too strung out on compliments\\nOver...   \n",
       "3  Are you alright?\\nI'm alright, I'm quite alrig...   \n",
       "4  That's true (That's true), that's right (That ...   \n",
       "\n",
       "                                             t-lyric  \n",
       "0  stir fry @@@ woo, woo, woo, woo\\nwoo, woo, woo...  \n",
       "1  drop it like it‚äôs hot @@@ snoop\\nsnoop\\n\\nwh...  \n",
       "2  headlines @@@ i might be too strung out on com...  \n",
       "3  xo tour llif3 @@@ are you alright?\\ni'm alrigh...  \n",
       "4  the way life goes @@@ that's true (that's true...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['t-lyric'] = data['title'] + \" @@@ \" + data['lyrics']\n",
    "data['t-lyric'] = data['t-lyric'].str.lower()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopChars = [',','(',')','.','-','[',']','\"']\n",
    "# preprocessing the corpus by converting all letters to lowercase, \n",
    "# replacing blank lines with blank string and removing special characters\n",
    "def preprocessText(text):\n",
    "#     text = text.replace('\\n', ' ').replace('\\t','')\n",
    "    processedText = text.lower()\n",
    "    for char in stopChars:\n",
    "        processedText = processedText.replace(char,'')\n",
    "    return processedText\n",
    "data['t-lyric'] = data['t-lyric'].apply(preprocessText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenization \n",
    "def corpusToList(corpus):\n",
    "    corpusList = [w for w in corpus.split(' ')] \n",
    "    corpusList = [i for i in corpusList if i] #removing empty strings from list\n",
    "    return corpusList\n",
    "data['t-lyric'] = data['t-lyric'].apply(corpusToList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>genre</th>\n",
       "      <th>title</th>\n",
       "      <th>lyrics</th>\n",
       "      <th>t-lyric</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Migos</td>\n",
       "      <td>rap</td>\n",
       "      <td>Stir Fry</td>\n",
       "      <td>Woo, woo, woo, woo\\nWoo, woo, woo, woo\\n\\nDanc...</td>\n",
       "      <td>[stir, fry, @@@, woo, woo, woo, woo\\nwoo, woo,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Snoop Dogg</td>\n",
       "      <td>rap</td>\n",
       "      <td>Drop It Like It‚Äôs Hot</td>\n",
       "      <td>Snoop\\nSnoop\\n\\nWhen the pimp's in the crib, m...</td>\n",
       "      <td>[drop, it, like, it‚äôs, hot, @@@, snoop\\nsnoo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Drake</td>\n",
       "      <td>rap</td>\n",
       "      <td>Headlines</td>\n",
       "      <td>I might be too strung out on compliments\\nOver...</td>\n",
       "      <td>[headlines, @@@, i, might, be, too, strung, ou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Lil Uzi Vert</td>\n",
       "      <td>rap</td>\n",
       "      <td>XO TOUR Llif3</td>\n",
       "      <td>Are you alright?\\nI'm alright, I'm quite alrig...</td>\n",
       "      <td>[xo, tour, llif3, @@@, are, you, alright?\\ni'm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Lil Uzi Vert</td>\n",
       "      <td>rap</td>\n",
       "      <td>The Way Life Goes</td>\n",
       "      <td>That's true (That's true), that's right (That ...</td>\n",
       "      <td>[the, way, life, goes, @@@, that's, true, that...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         artist genre                    title  \\\n",
       "0         Migos   rap                 Stir Fry   \n",
       "1    Snoop Dogg   rap  Drop It Like It‚Äôs Hot   \n",
       "2         Drake   rap                Headlines   \n",
       "3  Lil Uzi Vert   rap            XO TOUR Llif3   \n",
       "4  Lil Uzi Vert   rap        The Way Life Goes   \n",
       "\n",
       "                                              lyrics  \\\n",
       "0  Woo, woo, woo, woo\\nWoo, woo, woo, woo\\n\\nDanc...   \n",
       "1  Snoop\\nSnoop\\n\\nWhen the pimp's in the crib, m...   \n",
       "2  I might be too strung out on compliments\\nOver...   \n",
       "3  Are you alright?\\nI'm alright, I'm quite alrig...   \n",
       "4  That's true (That's true), that's right (That ...   \n",
       "\n",
       "                                             t-lyric  \n",
       "0  [stir, fry, @@@, woo, woo, woo, woo\\nwoo, woo,...  \n",
       "1  [drop, it, like, it‚äôs, hot, @@@, snoop\\nsnoo...  \n",
       "2  [headlines, @@@, i, might, be, too, strung, ou...  \n",
       "3  [xo, tour, llif3, @@@, are, you, alright?\\ni'm...  \n",
       "4  [the, way, life, goes, @@@, that's, true, that...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<map at 0x1215a5ed0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# trim each word for leading or trailing spaces / tabs.\n",
    "map(str.strip, data['t-lyric']) # trim words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>genre</th>\n",
       "      <th>title</th>\n",
       "      <th>lyrics</th>\n",
       "      <th>t-lyric</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Migos</td>\n",
       "      <td>rap</td>\n",
       "      <td>Stir Fry</td>\n",
       "      <td>Woo, woo, woo, woo\\nWoo, woo, woo, woo\\n\\nDanc...</td>\n",
       "      <td>[stir, fry, @@@, woo, woo, woo, woo\\nwoo, woo,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Snoop Dogg</td>\n",
       "      <td>rap</td>\n",
       "      <td>Drop It Like It‚Äôs Hot</td>\n",
       "      <td>Snoop\\nSnoop\\n\\nWhen the pimp's in the crib, m...</td>\n",
       "      <td>[drop, it, like, it‚äôs, hot, @@@, snoop\\nsnoo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Drake</td>\n",
       "      <td>rap</td>\n",
       "      <td>Headlines</td>\n",
       "      <td>I might be too strung out on compliments\\nOver...</td>\n",
       "      <td>[headlines, @@@, i, might, be, too, strung, ou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Lil Uzi Vert</td>\n",
       "      <td>rap</td>\n",
       "      <td>XO TOUR Llif3</td>\n",
       "      <td>Are you alright?\\nI'm alright, I'm quite alrig...</td>\n",
       "      <td>[xo, tour, llif3, @@@, are, you, alright?\\ni'm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Lil Uzi Vert</td>\n",
       "      <td>rap</td>\n",
       "      <td>The Way Life Goes</td>\n",
       "      <td>That's true (That's true), that's right (That ...</td>\n",
       "      <td>[the, way, life, goes, @@@, that's, true, that...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         artist genre                    title  \\\n",
       "0         Migos   rap                 Stir Fry   \n",
       "1    Snoop Dogg   rap  Drop It Like It‚Äôs Hot   \n",
       "2         Drake   rap                Headlines   \n",
       "3  Lil Uzi Vert   rap            XO TOUR Llif3   \n",
       "4  Lil Uzi Vert   rap        The Way Life Goes   \n",
       "\n",
       "                                              lyrics  \\\n",
       "0  Woo, woo, woo, woo\\nWoo, woo, woo, woo\\n\\nDanc...   \n",
       "1  Snoop\\nSnoop\\n\\nWhen the pimp's in the crib, m...   \n",
       "2  I might be too strung out on compliments\\nOver...   \n",
       "3  Are you alright?\\nI'm alright, I'm quite alrig...   \n",
       "4  That's true (That's true), that's right (That ...   \n",
       "\n",
       "                                             t-lyric  \n",
       "0  [stir, fry, @@@, woo, woo, woo, woo\\nwoo, woo,...  \n",
       "1  [drop, it, like, it‚äôs, hot, @@@, snoop\\nsnoo...  \n",
       "2  [headlines, @@@, i, might, be, too, strung, ou...  \n",
       "3  [xo, tour, llif3, @@@, are, you, alright?\\ni'm...  \n",
       "4  [the, way, life, goes, @@@, that's, true, that...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab length: 2810\n",
      "Unique words in corpus: 797\n"
     ]
    }
   ],
   "source": [
    "corpus_words = [x for sublist in data['t-lyric'] for x in sublist]\n",
    "vocab = sorted(set(corpus_words))\n",
    "print('vocab length:', len(corpus_words))\n",
    "print('Unique words in corpus: {}'.format(len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating numeric map; representing words with numberes \n",
    "# map specific number to each specific word of our corpus, and vice versa \n",
    "word2idx = {u: i for i, u in enumerate(vocab)}\n",
    "idx2words = np.array(vocab)\n",
    "word_as_int = np.array([word2idx[c] for c in corpus_words])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- User inputs a song title, and how many words they want the song to be. \n",
    "- Network does, for example, 100 predictions, and in the training phrase we know what word we need to generate. \n",
    "- (genre, song title); have a marker that it's the end of the title "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Character Level LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attempting a character level-RNN for a single genre\n",
    "https://www.kaggle.com/super13579/let-s-auto-write-the-deep-purple-lysics-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = pd.read_csv(\"sample-dataset.csv\")\n",
    "data1['lyrics']= data1['lyrics'].apply(preprocessText)\n",
    "print(data1.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DP_text = data1['lyrics'].str.cat(sep='\\n').lower()\n",
    "print('corpus length:', len(DP_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counting characters appeared in all lyrics\n",
    "chars = sorted(list(set(DP_text)))\n",
    "print(chars)\n",
    "print('total chars:', len(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary of characters, see the index of characters.\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "print(char_to_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 50 # The sentence window size\n",
    "step = 1 # The steps between the windows\n",
    "sentences = []\n",
    "next_chars = []\n",
    "\n",
    "# Create Target and sentences window\n",
    "for i in range(0, len(DP_text) - seq_length, step):\n",
    "    # range from current index to sequence length charaters\n",
    "    sentences.append(DP_text[i: i + seq_length])  \n",
    "    next_chars.append(DP_text[i + seq_length]) # the next character\n",
    "    \n",
    "sentences = np.array(sentences)\n",
    "next_chars = np.array(next_chars)\n",
    "\n",
    "#Print Sentence Window and next charaters\n",
    "print('Sentence Window')\n",
    "print (sentences[:5])\n",
    "print('Target characters')\n",
    "print (next_chars[:5])\n",
    "print('Number of sequences:', len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transferring the character to index \n",
    "def getdata(sentences, next_chars):\n",
    "    X = np.zeros((len(sentences),seq_length))\n",
    "    y = np.zeros((len(sentences)))\n",
    "    length = len(sentences)\n",
    "    index = 0\n",
    "    for i in range(len(sentences)):\n",
    "        sentence = sentences[i]\n",
    "        for t, char in enumerate(sentence):\n",
    "            X[i, t] = char_to_int[char]\n",
    "        y[i] = char_to_int[next_chars[i]]\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x,train_y = getdata(sentences, next_chars)\n",
    "print(train_x)\n",
    "print('Shape of training_x:', train_x.shape)\n",
    "print('Shape of training_y:', train_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building out the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class Simple_LSTM(nn.Module):\n",
    "    def __init__(self,n_vocab,hidden_dim, embedding_dim,dropout = 0.2):\n",
    "        super(Simple_LSTM, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim,dropout = dropout,num_layers = 2)\n",
    "        self.embeddings = nn.Embedding(n_vocab, embedding_dim)\n",
    "        self.fc = nn.Linear(hidden_dim, n_vocab)\n",
    "    \n",
    "    def forward(self, seq_in):\n",
    "        # for LSTM, input should be (Sequnce_length,batchsize,hidden_layer), so we need to transpose the input\n",
    "        embedded = self.embeddings(seq_in.t()) \n",
    "        lstm_out, _ = self.lstm(embedded)\n",
    "        # Only need to keep the last character \n",
    "        ht=lstm_out[-1] \n",
    "        out = self.fc(ht)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tensor = torch.tensor(train_x, dtype=torch.long)\n",
    "Y_train_tensor = torch.tensor(train_y, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "train = torch.utils.data.TensorDataset(X_train_tensor,Y_train_tensor)\n",
    "train_loader = torch.utils.data.DataLoader(train, batch_size = 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Simple_LSTM(47,256,256)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.002) # Using Adam optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time # Add time counter\n",
    "avg_losses_f = []\n",
    "n_epochs=10\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    print(\"Epoch: \", epoch)\n",
    "    start_time = time.time()\n",
    "    model.train()\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "    avg_loss = 0.\n",
    "    for i, (x_batch, y_batch) in enumerate(train_loader):\n",
    "        y_pred = model(x_batch)\n",
    "        \n",
    "        loss = loss_fn(y_pred, y_batch)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        avg_loss+= loss.item()/len(train_loader)\n",
    "        \n",
    "    elapsed_time = time.time() - start_time \n",
    "    print('Epoch {}/{} \\t loss={:.4f} \\t time={:.2f}s'.format(\n",
    "        epoch + 1, n_epochs, avg_loss, elapsed_time))\n",
    "    \n",
    "    avg_losses_f.append(avg_loss)    \n",
    "    \n",
    "print('All \\t loss={:.4f} \\t '.format(np.average(avg_losses_f)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(avg_losses_f)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "creating a function that can sample an index from a probability array \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "validate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the start sentence\n",
    "# sentence = 'i read in the news\\nthat the average man\\nplease kis'\n",
    "sentence = 'i put the new forgis on the g\\ni trap until the blo'\n",
    "variance = 0.25\n",
    "generated = ''\n",
    "original = sentence\n",
    "window = sentence\n",
    "\n",
    "for i in range(400):\n",
    "    x = np.zeros((1, seq_length))\n",
    "    for t, char in enumerate(window):\n",
    "        x[0, t] = char_to_int[char] # Change the sentence to index vector shape (1,50)\n",
    "        \n",
    "    x_in = Variable(torch.LongTensor(x))\n",
    "    pred = model(x_in)\n",
    "    pred = np.array(F.softmax(pred, dim=1).data[0].cpu())\n",
    "    next_index = sample(pred, variance)\n",
    "    next_char = int_to_char[next_index] # index to char\n",
    "\n",
    "    generated += next_char\n",
    "    window = window[1:] + next_char # Update Window for next char predict\n",
    "    \n",
    "print(original + generated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Level LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         artist genre                    title  \\\n",
      "0         Migos   rap                 Stir Fry   \n",
      "1    Snoop Dogg   rap  Drop It Like It‚Äôs Hot   \n",
      "2         Drake   rap                Headlines   \n",
      "3  Lil Uzi Vert   rap            XO TOUR Llif3   \n",
      "4  Lil Uzi Vert   rap        The Way Life Goes   \n",
      "\n",
      "                                              lyrics  \n",
      "0  woo woo woo woo\\nwoo woo woo woo\\n\\ndance with...  \n",
      "1  snoop\\nsnoop\\n\\nwhen the pimp's in the crib ma...  \n",
      "2  i might be too strung out on compliments\\nover...  \n",
      "3  are you alright?\\ni'm alright i'm quite alrigh...  \n",
      "4  that's true that's true that's right that righ...  \n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "#nltk.download('punkt')\n",
    "data1 = pd.read_csv(\"sample-dataset.csv\")\n",
    "data1['lyrics']= data1['lyrics'].apply(preprocessText)\n",
    "print(data1.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus length: 3440\n"
     ]
    }
   ],
   "source": [
    "DP_text = word_tokenize(data1['lyrics'].str.cat(sep='\\n').lower())\n",
    "print('corpus length:', len(DP_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total words: 650\n"
     ]
    }
   ],
   "source": [
    "# Counting characters appeared in all lyrics\n",
    "words = sorted(list(set(DP_text)))\n",
    "print('total words:', len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary of characters, see the index of characters.\n",
    "char_to_int = dict((c, i) for i, c in enumerate(words))\n",
    "int_to_char = dict((i, c) for i, c in enumerate(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence Window\n",
      "[['woo' 'woo' 'woo' 'woo' 'woo' 'woo' 'woo' 'woo' 'dance' 'with']\n",
      " ['woo' 'woo' 'woo' 'woo' 'woo' 'woo' 'woo' 'dance' 'with' 'my']\n",
      " ['woo' 'woo' 'woo' 'woo' 'woo' 'woo' 'dance' 'with' 'my' 'dogs']\n",
      " ['woo' 'woo' 'woo' 'woo' 'woo' 'dance' 'with' 'my' 'dogs' 'in']\n",
      " ['woo' 'woo' 'woo' 'woo' 'dance' 'with' 'my' 'dogs' 'in' 'the']]\n",
      "Target characters\n",
      "['my' 'dogs' 'in' 'the' 'nighttime']\n",
      "Number of sequences: 3430\n"
     ]
    }
   ],
   "source": [
    "seq_length = 10 # The sentence window size\n",
    "step = 1 # The steps between the windows\n",
    "sentences = []\n",
    "next_chars = []\n",
    "\n",
    "# Create Target and sentences window\n",
    "for i in range(0, len(DP_text) - seq_length, step):\n",
    "    # range from current index to sequence length charaters\n",
    "    sentences.append(DP_text[i: i + seq_length])  \n",
    "    next_chars.append(DP_text[i + seq_length]) # the next character\n",
    "    \n",
    "sentences = np.array(sentences)\n",
    "next_chars = np.array(next_chars)\n",
    "\n",
    "#Print Sentence Window and next charaters\n",
    "print('Sentence Window')\n",
    "print (sentences[:5])\n",
    "print('Target characters')\n",
    "print (next_chars[:5])\n",
    "print('Number of sequences:', len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transferring the character to index \n",
    "def getdata(sentences, next_chars):\n",
    "    X = np.zeros((len(sentences),seq_length))\n",
    "    y = np.zeros((len(sentences)))\n",
    "    length = len(sentences)\n",
    "    index = 0\n",
    "    for i in range(len(sentences)):\n",
    "        sentence = sentences[i]\n",
    "        for t, char in enumerate(sentence):\n",
    "            X[i, t] = char_to_int[char]\n",
    "        y[i] = char_to_int[next_chars[i]]\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[632. 632. 632. ... 632. 145. 629.]\n",
      " [632. 632. 632. ... 145. 629. 364.]\n",
      " [632. 632. 632. ... 629. 364. 162.]\n",
      " ...\n",
      " [ 86. 284. 320. ... 284. 628. 284.]\n",
      " [284. 320. 547. ... 628. 284. 374.]\n",
      " [320. 547. 236. ... 284. 374. 348.]]\n",
      "Shape of training_x: (3430, 10)\n",
      "Shape of training_y: (3430,)\n"
     ]
    }
   ],
   "source": [
    "train_x,train_y = getdata(sentences, next_chars)\n",
    "print(train_x)\n",
    "print('Shape of training_x:', train_x.shape)\n",
    "print('Shape of training_y:', train_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class Simple_LSTM(nn.Module):\n",
    "    def __init__(self,n_vocab,hidden_dim, embedding_dim,dropout = 0.2):\n",
    "        super(Simple_LSTM, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim,dropout = dropout,num_layers = 2)\n",
    "        self.embeddings = nn.Embedding(n_vocab, embedding_dim)\n",
    "        self.fc = nn.Linear(hidden_dim, n_vocab)\n",
    "    \n",
    "    def forward(self, seq_in):\n",
    "        # for LSTM, input should be (Sequnce_length,batchsize,hidden_layer), so we need to transpose the input\n",
    "        embedded = self.embeddings(seq_in.t()) \n",
    "        lstm_out, _ = self.lstm(embedded)\n",
    "        # Only need to keep the last character \n",
    "        ht=lstm_out[-1] \n",
    "        out = self.fc(ht)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training_x: torch.Size([3430, 10])\n",
      "Shape of training_y: torch.Size([3430])\n"
     ]
    }
   ],
   "source": [
    "X_train_tensor = torch.tensor(train_x, dtype=torch.long)\n",
    "Y_train_tensor = torch.tensor(train_y, dtype=torch.long)\n",
    "print('Shape of training_x:', X_train_tensor.shape)\n",
    "print('Shape of training_y:', Y_train_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "train = torch.utils.data.TensorDataset(X_train_tensor,Y_train_tensor)\n",
    "train_loader = torch.utils.data.DataLoader(train, batch_size = 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Simple_LSTM(len(words),256,256)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.002) # Using Adam optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0\n",
      "Epoch 1/20 \t loss=5.8476 \t time=2.91s\n",
      "Epoch:  1\n",
      "Epoch 2/20 \t loss=5.3796 \t time=2.88s\n",
      "Epoch:  2\n",
      "Epoch 3/20 \t loss=4.9190 \t time=2.89s\n",
      "Epoch:  3\n",
      "Epoch 4/20 \t loss=4.3490 \t time=2.92s\n",
      "Epoch:  4\n",
      "Epoch 5/20 \t loss=3.7661 \t time=3.18s\n",
      "Epoch:  5\n",
      "Epoch 6/20 \t loss=3.2207 \t time=3.63s\n",
      "Epoch:  6\n",
      "Epoch 7/20 \t loss=2.7175 \t time=3.16s\n",
      "Epoch:  7\n",
      "Epoch 8/20 \t loss=2.2093 \t time=3.18s\n",
      "Epoch:  8\n",
      "Epoch 9/20 \t loss=1.7566 \t time=3.23s\n",
      "Epoch:  9\n",
      "Epoch 10/20 \t loss=1.3615 \t time=3.42s\n",
      "Epoch:  10\n",
      "Epoch 11/20 \t loss=1.0483 \t time=3.21s\n",
      "Epoch:  11\n",
      "Epoch 12/20 \t loss=0.7791 \t time=3.11s\n",
      "Epoch:  12\n",
      "Epoch 13/20 \t loss=0.5924 \t time=3.12s\n",
      "Epoch:  13\n",
      "Epoch 14/20 \t loss=0.4422 \t time=3.11s\n",
      "Epoch:  14\n",
      "Epoch 15/20 \t loss=0.3324 \t time=3.07s\n",
      "Epoch:  15\n",
      "Epoch 16/20 \t loss=0.2551 \t time=3.14s\n",
      "Epoch:  16\n",
      "Epoch 17/20 \t loss=0.1913 \t time=3.29s\n",
      "Epoch:  17\n",
      "Epoch 18/20 \t loss=0.1563 \t time=3.19s\n",
      "Epoch:  18\n",
      "Epoch 19/20 \t loss=0.1397 \t time=3.08s\n",
      "Epoch:  19\n",
      "Epoch 20/20 \t loss=0.1239 \t time=3.22s\n",
      "All \t loss=1.9794 \t \n"
     ]
    }
   ],
   "source": [
    "import time # Add time counter\n",
    "avg_losses_f = []\n",
    "n_epochs=20\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    print(\"Epoch: \", epoch)\n",
    "    start_time = time.time()\n",
    "    model.train()\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "    avg_loss = 0.\n",
    "    for i, (x_batch, y_batch) in enumerate(train_loader):\n",
    "        y_pred = model(x_batch)\n",
    "        \n",
    "        loss = loss_fn(y_pred, y_batch)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        avg_loss+= loss.item()/len(train_loader)\n",
    "        \n",
    "    elapsed_time = time.time() - start_time \n",
    "    print('Epoch {}/{} \\t loss={:.4f} \\t time={:.2f}s'.format(\n",
    "        epoch + 1, n_epochs, avg_loss, elapsed_time))\n",
    "    \n",
    "    avg_losses_f.append(avg_loss)    \n",
    "    \n",
    "print('All \\t loss={:.4f} \\t '.format(np.average(avg_losses_f)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(avg_losses_f)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "she got forever addicted to a body killer spillin ice like that she do n't want me then i'ma let her bye go over there with that broke fella go over walk off my saint laurent that leather yeah my new chick i swear that she better ooh want me back never ooh that 's true that 's true that 's right that right that right she 's sipping mo√´t and yeah i swear it gets her wetter my louboutins new so my bottoms they is redder yeah it 's red no i 'm not a rat but i 'm all about my cheddar 'bout my bread just talked to your homie she said we should be together yeah gave me brain was so insane that i made her my header woah if she ever call my phone you know i got ta dead her but i like that girl too much i wish i never met her i know it hurts sometimes woah but you 'll get over it but why ? you 'll find another life to live you 'll find i swear that you 'll get over it you 'll get over it i know it hurts sometimes but you 'll get over it yeah you 'll find another life to live i swear that you 'll get over it i know you 're sad and tired you 've got nothing left to give yeah you 'll find another life to live yeah i know that you 'll get over it i know you 're sad and tired you 've got nothing left to give yeah you 'll find another life to live i swear that you 'll get over it and i know you 're sad and tired you 've got nothing left to give yeah you 'll find another life to live yeah i know that you 'll get over it i tied up my raf you strapped up your rick diamonds on your neck ice all on my wrist complement my style she do n't want me i 'm running wild you know i respect her on that level she do n't want me then i'ma let her bye go over there with that broke fella go over walk off my saint laurent that leather yeah my new chick i swear that she better ooh want me back never ooh that 's true that 's true that 's right that right that right she\n"
     ]
    }
   ],
   "source": [
    "# Define the start sentence\n",
    "# sentence = 'i read in the news\\nthat the average man\\nplease kis'\n",
    "sentence = [\"she\", \"got\", \"forever\", \"addicted\", \"to\", \"a\", \"body\", \"killer\", \"spillin\", \"ice\"]\n",
    "variance = 0.25\n",
    "generated = []\n",
    "original = sentence\n",
    "window = sentence\n",
    "\n",
    "for i in range(400):\n",
    "    x = np.zeros((1, seq_length))\n",
    "    for t, char in enumerate(window):\n",
    "        x[0, t] = char_to_int[char] # Change the sentence to index vector shape (1,50)\n",
    "        \n",
    "    x_in = Variable(torch.LongTensor(x))\n",
    "    pred = model(x_in)\n",
    "    pred = np.array(F.softmax(pred, dim=1).data[0].cpu())\n",
    "    next_index = sample(pred, variance)\n",
    "    next_char = int_to_char[next_index] # index to char\n",
    "\n",
    "    generated = generated + [next_char]\n",
    "    window = window[1:] + [next_char] # Update Window for next char predict\n",
    "    \n",
    "print(\" \".join(original + generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
