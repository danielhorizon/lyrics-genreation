{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN POS Tagger\n",
    "\n",
    "A *part of speech tagger* (POS tagger) takes a sentence and labels each word with its part of speech (POS).\n",
    "\n",
    "For example:\n",
    "- **Input:** The cat saw the dog on the bench.\n",
    "- **Output:** DET NOUN VERB DET NOUN ADP DET NOUN PUNCT\n",
    "\n",
    "In this part of the lecture, we will create a POS tagger using an SRN architecture.\n",
    "\n",
    "Import the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torchtext import data\n",
    "from torchtext.datasets import UDPOS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Definition\n",
    "\n",
    "<img src=\"images/srn.png\" style=\"width: 50%; height: 50%;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNPOSTagger(nn.Module):\n",
    "    def __init__(self, x_vocab, y_vocab, embedding_size, hidden_size):\n",
    "        super(RNNPOSTagger, self).__init__()\n",
    "        self._embedding = nn.Embedding(len(x_vocab), embedding_size)\n",
    "        self._rnn = nn.RNN(embedding_size, hidden_size)\n",
    "        self._linear = nn.Linear(hidden_size, len(y_vocab))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        embeddings = self._embedding(x)\n",
    "        rnn_output, _ = self._rnn(embeddings)\n",
    "        return self._linear(rnn_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "Prepare the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading en-ud-v2.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "en-ud-v2.zip: 100%|██████████| 688k/688k [00:00<00:00, 738kB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracting\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "UDPOS.download(\"data\")\n",
    "x_field = data.Field()\n",
    "y_field = data.Field()\n",
    "fields = [(\"x_field\", x_field), (\"y_field\", y_field)]\n",
    "train_data, val_data, test_data = UDPOS.splits(fields, root=\"data\")\n",
    "\n",
    "# Build the vocab\n",
    "x_field.build_vocab(train_data, val_data, test_data)\n",
    "y_field.build_vocab(train_data, val_data, test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNNPOSTagger(x_field.vocab, y_field.vocab, 50, 50)\n",
    "loss_function = nn.CrossEntropyLoss(ignore_index=1)\n",
    "optimizer = optim.Adam(model.parameters(), lr=.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 10 Loss: 77.078, Accuracy: 0.350\n",
      "Batch 20 Loss: 59.508, Accuracy: 0.504\n",
      "Batch 30 Loss: 64.290, Accuracy: 0.549\n",
      "Batch 40 Loss: 55.345, Accuracy: 0.623\n",
      "Batch 50 Loss: 118.026, Accuracy: 0.611\n",
      "Batch 60 Loss: 68.761, Accuracy: 0.613\n",
      "Batch 70 Loss: 53.593, Accuracy: 0.634\n",
      "Batch 80 Loss: 70.788, Accuracy: 0.728\n",
      "Batch 90 Loss: 62.213, Accuracy: 0.647\n",
      "Batch 100 Loss: 90.538, Accuracy: 0.690\n",
      "Batch 110 Loss: 31.041, Accuracy: 0.657\n",
      "Batch 120 Loss: 140.014, Accuracy: 0.701\n",
      "Batch 130 Loss: 44.190, Accuracy: 0.747\n",
      "Batch 140 Loss: 47.666, Accuracy: 0.711\n",
      "Batch 150 Loss: 44.179, Accuracy: 0.712\n",
      "Batch 160 Loss: 51.545, Accuracy: 0.762\n",
      "Batch 170 Loss: 31.979, Accuracy: 0.758\n",
      "Batch 180 Loss: 49.518, Accuracy: 0.755\n",
      "Batch 190 Loss: 29.433, Accuracy: 0.776\n",
      "Batch 200 Loss: 35.392, Accuracy: 0.792\n",
      "Batch 210 Loss: 29.950, Accuracy: 0.820\n",
      "Batch 220 Loss: 28.512, Accuracy: 0.804\n",
      "Batch 230 Loss: 43.355, Accuracy: 0.774\n",
      "Batch 240 Loss: 64.275, Accuracy: 0.823\n",
      "Batch 250 Loss: 29.085, Accuracy: 0.814\n",
      "Batch 260 Loss: 42.307, Accuracy: 0.784\n",
      "Batch 270 Loss: 46.000, Accuracy: 0.806\n",
      "Batch 280 Loss: 18.552, Accuracy: 0.813\n",
      "Batch 290 Loss: 39.377, Accuracy: 0.807\n",
      "Batch 300 Loss: 26.654, Accuracy: 0.822\n",
      "Batch 310 Loss: 25.008, Accuracy: 0.811\n",
      "Batch 320 Loss: 18.269, Accuracy: 0.830\n",
      "Batch 330 Loss: 19.033, Accuracy: 0.837\n",
      "Batch 340 Loss: 37.024, Accuracy: 0.823\n",
      "Batch 350 Loss: 25.475, Accuracy: 0.811\n",
      "Batch 360 Loss: 21.980, Accuracy: 0.853\n",
      "Batch 370 Loss: 41.012, Accuracy: 0.841\n",
      "Batch 380 Loss: 28.918, Accuracy: 0.842\n",
      "Batch 390 Loss: 14.815, Accuracy: 0.850\n"
     ]
    }
   ],
   "source": [
    "ctr = 0\n",
    "for batch in data.BucketIterator(train_data, 32):\n",
    "    # Forward pass\n",
    "    y_hat = model(batch.x_field)\n",
    "    loss = 0\n",
    "    for t in range(len(y_hat)):\n",
    "        loss += loss_function(y_hat[t], batch.y_field[t])\n",
    "\n",
    "    # Backward pass\n",
    "    model.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Display progress\n",
    "    ctr += 1\n",
    "    if ctr % 10 != 0:\n",
    "        continue\n",
    "\n",
    "    num_correct = (y_hat.argmax(axis=2) == batch.y_field).sum()\n",
    "    num_total = (batch.y_field != 1).sum()\n",
    "    acc = float(num_correct) / float(num_total)\n",
    "    print(\"Batch {} Loss: {:.3f}, Accuracy: {:.3f}\".format(ctr, loss, acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Loss: 1579.206, Testing Accuracy: 0.819\n"
     ]
    }
   ],
   "source": [
    "loss = 0\n",
    "num_correct = 0\n",
    "num_total = 0\n",
    "for batch in data.BucketIterator(test_data, 32):\n",
    "    y_hat = model(batch.x_field)\n",
    "    for t in range(len(y_hat)):\n",
    "        loss += loss_function(y_hat[t], batch.y_field[t])\n",
    "    num_correct += (y_hat.argmax(axis=2) == batch.y_field).sum()\n",
    "    num_total += (batch.y_field != 1).sum()\n",
    "\n",
    "acc = float(num_correct) / float(num_total)\n",
    "print(\"Testing Loss: {:.3f}, Testing Accuracy: {:.3f}\".format(loss, acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
